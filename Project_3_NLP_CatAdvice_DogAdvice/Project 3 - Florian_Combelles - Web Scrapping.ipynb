{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d91da0f",
   "metadata": {},
   "source": [
    "# Project 3 Web APIs & NLP - Florian Combelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12081c",
   "metadata": {},
   "source": [
    "In week four we've learned about a few different classifiers. In week five we'll learn about webscraping, APIs, and Natural Language Processing (NLP). This project will put those skills to the test.\n",
    "\n",
    "For project 3, your goal is two-fold:\n",
    "1. Using [Pushshift's](https://github.com/pushshift/api) API, you'll collect posts from two subreddits of your choosing.\n",
    "2. You'll then use NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem.\n",
    "\n",
    "###### Requirements\n",
    "\n",
    "- Gather and prepare your data using the `requests` library.\n",
    "- **Create and compare two models**. One of these must be a Naive Bayes classifier, however the other can be a classifier of your choosing: logistic regression, KNN, SVM, etc.\n",
    "- A Jupyter Notebook with your analysis for a peer audience of data scientists.\n",
    "- An executive summary of your results.\n",
    "- A short presentation outlining your process and findings for a semi-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55edbc6",
   "metadata": {},
   "source": [
    "## Problem Statement and background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c5dbd",
   "metadata": {},
   "source": [
    "###### Problem Statement\n",
    "\n",
    "During Covid 19, we have seen an increase in the number of new pet owners in Singapore.\n",
    "These new inexperienced pet owners are facing a lack of ressources on how to care for their new pets.\n",
    "\n",
    "This leads to them having to overrely on veterinarians and pet stores to provides information and respond to their queries.\n",
    "\n",
    "This influx of inexperienced pets owners overly reliant on vets and pet store reduces their work efficiency and distract them from their main responsibilites\n",
    "\n",
    "\n",
    "##### Background\n",
    "\n",
    "We are working for a company called Pet Smart.\n",
    "\n",
    "We are releasing a new mobile app that includes two features:\n",
    "* A chat box where you can ask your cat or dog related question and get an answer from a team of experts.\n",
    "* Articles that provides informations and tips on how to care for your pets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e9b92",
   "metadata": {},
   "source": [
    "## Part 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991521ce",
   "metadata": {},
   "source": [
    "For this project, we will be scrapping [Reddit](https://www.reddit.com) and particularly two similar subreddits:\n",
    "* [DogAdvice](https://www.reddit.com/r/DogAdvice/)\n",
    "* [CatAdvice](https://www.reddit.com/r/CatAdvice/)\n",
    "\n",
    "###### These two subreddits are a place where people can post and look for information/advice regarding their dogs and cats.\n",
    "\n",
    "In order to acquire information from these two subreddits, we will be using the [Pushshift's](https://github.com/pushshift/api) API.\n",
    "\n",
    "This API was designed to help provide enhanced functionality and search capabilities for searching Reddit comments and submissions.\n",
    "\n",
    "It provides full functionality for searching Reddit data and also includes the capability of creating powerful data aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561f8e5",
   "metadata": {},
   "source": [
    "##### Importing Libraries\n",
    "\n",
    "For our data acquisition we only need to import two libraries:\n",
    "* Requests for our Data Scrapping\n",
    "* Pandas to transform this data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db57530",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a81c7",
   "metadata": {},
   "source": [
    "##### Setting up the url to retrieve subreddit content\n",
    "\n",
    "Pushshift provides two options when scrapping Reddit.\n",
    "* Submissions - Scrape posts content\n",
    "* Comments - Scrape comments on posts\n",
    "\n",
    "For this project, we decided to focus on submission as we want to identify in which subreddit a particular submission would fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a78c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicating base url to retrieve Reddit Posts\n",
    "\n",
    "url = f'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5fa55",
   "metadata": {},
   "source": [
    "##### Creating parameters variables\n",
    "\n",
    "We are specifying the subreddits we want to collect information from (CatAdvice, DogAdvice)\n",
    "\n",
    "We are also creating a variable that will help us automate data collection beyond the limitation of 250 posts per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a lists of subreddits to scrape\n",
    "# Creating a counter to define how many times we want to go through the posts (since we have a limit of 250)\n",
    "\n",
    "subreddits = ['CatAdvice', 'DogAdvice']\n",
    "max_posts = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b556b6",
   "metadata": {},
   "source": [
    "##### Scraping Reddit content and saving results to a CSV\n",
    "\n",
    "We are looking to get 1000 posts from each subreddits.\n",
    "\n",
    "In order to do so, we are creating two loops.\n",
    "* A for loop that will filter which information (parameters) we want to retrieve.\n",
    "\n",
    "In this case, we will be collected the title, time of post creation, domain (is the post a new content or is it shared from another site?) and the type of content that the post includes (photo, videos...).\n",
    "We also restricts only to posts that we not removed/deleted.\n",
    "\n",
    "* A while loop\n",
    "\n",
    "The while loop will look for the posts that match our criterias and retrieve their content as a json file.\n",
    "This json file will then be converted into an entry into a dataframe that will contain all the posts for each subreddit.\n",
    "Finally, because of the limit of 250 posts, we added a counter that will allow us to repeat the process as many time as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a For Loop to go through our parameters\n",
    "\n",
    "for i in subreddits:\n",
    "    params = {'subreddit': i,\n",
    "         'size': '250',\n",
    "# Adding a new criteria: selftext:not 'removed' to filter out the removed posts              \n",
    "         'selftext:not': '[removed]',     \n",
    "         'fields': ['selftext', 'title', 'subreddit', 'created_utc',\n",
    "                    'domain', 'is_robot_indexable', 'post_hint']}\n",
    "    \n",
    "    count = 1\n",
    "    total_submissions = []\n",
    "\n",
    "# Creating a While loop to get the json response and save them in a DatFrame\n",
    "# Add a new before parameter with the date of the oldest post scrapped\n",
    "# Increment counter and start scraping again from oldest post\n",
    "\n",
    "    while count < max_posts:\n",
    "        response = requests.get(url, params)\n",
    "        resp_json = response.json()\n",
    "        posts = resp_json['data']\n",
    "        submissions = pd.DataFrame(posts)\n",
    "        total_submissions.append(submissions)\n",
    "        params['before'] = submissions['created_utc'].iloc[-1]\n",
    "        count += 1\n",
    "\n",
    "# Concatenate all the submissions into a single DataFrame\n",
    "# Save our two subreddit into a csv file\n",
    "        \n",
    "    final_submissions = pd.concat(total_submissions)    \n",
    "    final_submissions.to_csv(f'data/{i}.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f6f39",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that we have managed to automate acquisition and retrieve the data from our two subreddit, we can move on the Data Cleaning and EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
